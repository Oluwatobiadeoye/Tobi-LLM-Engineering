{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08636aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Enum):\n",
    "    GPT_4o_MINI = 'gpt-4o-mini'\n",
    "    LLAMA_3_2 = 'llama3.2'\n",
    "\n",
    "class ModelClient:\n",
    "    def __init__(self, model: str, **kwargs):\n",
    "        self.client = OpenAI(**kwargs)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, messages, **kwargs):\n",
    "        return self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_ai_client(model: Model) -> ModelClient:\n",
    "    match model:\n",
    "        case Model.GPT_4o_MINI:\n",
    "            return ModelClient(model=Model.GPT_4o_MINI.value)\n",
    "        case Model.LLAMA_3_2:\n",
    "            return ModelClient(model=Model.LLAMA_3_2.value, base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a tutor expert at providing answers to technical question in a detailed manner. \\\n",
    "Your goal is to provide answers that helps your syudents build understanding of any question you are asked \\\n",
    "by providing beginner friendly explanations with relevant examples. In addition, you are able to suggests \\\n",
    "further topics or references to solidify their understanding. The question could be code snippet that the \\\n",
    "student is trying to understand what it does. Ensure your answers are broken down in a markdown manner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt():\n",
    "    question = input('Enter your question: ')\n",
    "    return f\"Please provide answer to this question to the best of your knowledge: {question}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(model: Model, user_prompt, stream=False):\n",
    "    model_client = get_open_ai_client(model);\n",
    "\n",
    "    response = model_client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ],\n",
    "    stream=stream\n",
    "    )   \n",
    "\n",
    "    if stream:\n",
    "        stream_answer(response)\n",
    "    else:\n",
    "        answer = response.choices[0].message.content\n",
    "        display(Markdown(answer))\n",
    "\n",
    "\n",
    "def stream_answer(stream_response):\n",
    "    answer = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream_response:\n",
    "        answer += chunk.choices[0].delta.content or ''\n",
    "        answer = answer.replace(\"```\",\"\")\n",
    "        update_display(Markdown(answer), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = get_user_prompt()\n",
    "\n",
    "display(Markdown(\"## **Response from GPT 4o Mini**\")) \n",
    "gpt_4o_mini_response = answer_question(model=Model.GPT_4o_MINI, user_prompt=user_question, stream=True)\n",
    "display(Markdown(\"## **Response from GPT 4o Mini ends here**\")) \n",
    "\n",
    "display(Markdown(\"## **Response from LLAMA 3.2**\")) \n",
    "llama_response = answer_question(model=Model.LLAMA_3_2, user_prompt=user_question, stream=True)\n",
    "display(Markdown(\"## **Response from LLAMA 3.2 ends here**\")) \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
